{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch NN Workshop\n",
    "### Introduction\n",
    "Welcome to the PSIML's PyTorch Neural Networks Workshop. \n",
    "### Goal\n",
    "In this workshop you will build a deep learning pipeline using PyTorch. You will use datasets and dataloaders to manipulate the data. You will learn how to make new models or change existing ones. You will make a 'training loop' to train, validate and test your model. \n",
    "### Key Ingredients\n",
    "* Data\n",
    "* Models\n",
    "    * Logistic Regression as a single layer NN\n",
    "    * Multi-layer NN\n",
    "* Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "I1"
    ]
   },
   "outputs": [],
   "source": [
    "# some useful imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "You have already learned how to make PyTorch datasets and dataloaders in previous workshop, so this time we will use some pre-made ones to speed things up.\n",
    "\n",
    "Head out to [torchvision/datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) and check out the [Fashion-MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist) dataset.\n",
    "\n",
    "This is a 'map style' dataset, therefore, `dataset[idx]` will return a sample at that index in form of a tuple (image, label) similar to the dataset example you saw in the PyTorch lecture.\n",
    "\n",
    "You will notice that only train and test parts of the dataset are available. Therefore, we need to split the training part into train and validation. For this purpose it is useful to check the [SubsetRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler). Then, we will provide the train and valid split indices to create the corresponding samplers that we will pass to the train and valid dataloaders.\n",
    "\n",
    "Note that if you wish to add some augmentations to your training dataset, you will need to create separate datasets. In the cell below, we create only the dummy transformations to convert our data into tensors. If you wish to play with data augmentations check [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html), however, it is optional and you can complete the workshop without any augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D1"
    ]
   },
   "outputs": [],
   "source": [
    "# data preprocessing and augmentations\n",
    "# you can leave these as is for now (ToTensor just converts a PIL image to torch.tensor)\n",
    "train_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])  # define your train transforms = augmentation + normalization\n",
    "valid_test_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])  # define your validation/test transforms = usually just normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D2"
    ]
   },
   "outputs": [],
   "source": [
    "# get some datasets\n",
    "train_dataset = torchvision.datasets.FashionMNIST('./data', train=True, transform=train_transforms, download=True)\n",
    "valid_dataset = torchvision.datasets.FashionMNIST('./data', train=True, transform=valid_test_transforms, download=True)\n",
    "    # note that we make the valid dataset using same data as for the train, but with different transformations.\n",
    "    # we will later split the indices to form the actual training and validation datasets\n",
    "test_dataset = torchvision.datasets.FashionMNIST('./data', train=False, transform=valid_test_transforms, download=True)\n",
    "\n",
    "# get the labels of the dataset\n",
    "label_descriptions = {i:train_dataset.classes[i] for i in range(len(train_dataset.classes))}\n",
    "for L in label_descriptions:\n",
    "    print(f'{L}: {label_descriptions[L]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D3"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "# What are the dimensionalities of our dataset?\n",
    "img_size = None  # input dimensions\n",
    "num_classes = None  # number of classes in our dataset\n",
    "\n",
    "print(f'input dim: {img_size}   output dim: {num_classes}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "input dim: torch.Size([1, 28, 28])   output dim: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D4"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# Split training dataset into training and test split and balance classes in both splits\n",
    "# make train and valid splits\n",
    "\n",
    "import random\n",
    "random.seed(0)  # rng seed, set to 0 for reproducibility\n",
    "train_dataset_indices = list(range(len(train_dataset)))\n",
    "\n",
    "label_to_indices = dict()  # dictionary in which key is label and value is list of indices which belongs to that label\n",
    "\n",
    "# initialize label_to_indices\n",
    "for idx in range(len(label_descriptions)):\n",
    "    label_to_indices[idx] = list()\n",
    "\n",
    "for idx in range(len(train_dataset)):\n",
    "    _, label = None\n",
    "    label_to_indices[label].append(idx)  \n",
    "    \n",
    "valid_split_indices = []\n",
    "validation_split_ratio = 0.2  # 20% of training instances \n",
    "# sample 20% instances for validation\n",
    "for key in label_to_indices.keys():\n",
    "    valid_split_indices.extend(np.random.choice(None), replace=False))\n",
    "\n",
    "# remove validation instances from train split\n",
    "train_split_indices = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "[24413, 42772, 6584, 51319, 6641, 34757, 4076, 50822, 42060, 7211]\n",
    "[3076, 45859, 6146, 39569, 32109, 18599, 16469, 28572, 19340, 34134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D5"
    ]
   },
   "outputs": [],
   "source": [
    "# we can use SubsetRandomSampler to limit which indices our dataloader will use \n",
    "# (even though we are passing the same dataset to both of the dataloaders)\n",
    "train_subset_sampler = torch.utils.data.SubsetRandomSampler(train_split_indices)\n",
    "valid_subset_sampler = torch.utils.data.SubsetRandomSampler(valid_split_indices)\n",
    "\n",
    "# make dataloaders\n",
    "batch_size = 1 \n",
    "num_workers = 0 \n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_subset_sampler, num_workers=0)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, sampler=valid_subset_sampler, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D6"
    ]
   },
   "outputs": [],
   "source": [
    "# lets visualize some data from our datasets\n",
    "\n",
    "for dl in [train_dataloader, valid_dataloader, test_dataloader]:\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    idl = iter(dl)  # so that we can call 'next'\n",
    "    for i in range(10):\n",
    "        image,label = next(idl)\n",
    "        plt.subplot(1, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(label_descriptions[label.item()].title())\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression as a single layer NN\n",
    "In this section we will write the simple logistic regression model using available PyTorch functionalities.\n",
    "\n",
    "First, take a look at https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression.\n",
    "\n",
    "Therefore, we can use a [Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) model plus the [Cross Entropy Loss](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) to define the multi-class logistic regression optimization problem.\n",
    "\n",
    "PyTorch models are based on the [torch.nn.Module](https://pytorch.org/docs/master/generated/torch.nn.Module.html) class. It defines the model components ('\\_\\_init\\_\\_') and the operations applied to the input in terms of those components to get the output ('forward').\n",
    "\n",
    "For example:\n",
    "```Python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=100, out_features=10, bias=True)\n",
    "        self.layer2 = torch.nn.Linear(in_features=10, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "M1"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# define a model class with a single linear layer with bias\n",
    "# we do not need the softmax activation function since  \n",
    "# the CELoss that we'll use later performs the softmax for us\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)  # rng for reproducibility\n",
    "\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layer1 = None  # make a linear layer with bias; \n",
    "                            # input is the flattened image\n",
    "                            # and we need 1 output per class\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = torch.ones(28*28)\n",
    "    model = LinearModel()\n",
    "    print(model(image))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([-0.0854, -0.3401,  0.2807, -0.2396, -0.8881, -0.3894, -0.1590,  0.3492,\n",
    "         0.7761, -0.1154])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "We will train a logistic regression model on Fashion-MNIST data, applying all the things we have learned so far. We will provide you with the code (similar to what you have seen in the logistic regression workshop) that you can imediately play with.\n",
    "\n",
    "We will use the ready-made PyTorch dataset for Fashion-MNIST from the torchvision package.\n",
    "\n",
    "Try playing with the following parameters and see if you can get better results or speed-up:\n",
    "- learning_rate\n",
    "- device (NOTE: you will need a gpu with cuda in order to play with this parameter)\n",
    "- batch_size\n",
    "- num_epochs\n",
    "- num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "TL1"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the meta parameters of training\n",
    "\n",
    "# create the model\n",
    "model = LinearModel()  # our linear model\n",
    "\n",
    "# create the SGD optimizer \n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # we will use stochastic gradient descent\n",
    "    \n",
    "# create the CrossEntropy Loss function\n",
    "loss_func = nn.CrossEntropyLoss()  # we will use cross entropy loss\n",
    "\n",
    "# make dataloaders\n",
    "num_epochs = 2\n",
    "batch_size = 1000 \n",
    "num_workers = 0\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                                               sampler=train_subset_sampler, num_workers=num_workers)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, \n",
    "                                               sampler=valid_subset_sampler, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, \n",
    "                                              shuffle=False, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "TL2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "import time\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")  # change \"cpu\" to \"cuda\" to run on your GPU\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()  # sets the model into training mode.\n",
    "                   # (important if you use batch_norm, droupout and similar layers \n",
    "                   # that behave differently during training and during evaluation)\n",
    "            \n",
    "    for i, (x,y) in enumerate(train_dataloader):\n",
    "        x = None  # flatten 'x' and move it to device\n",
    "        y = None  # move 'y' to device\n",
    "\n",
    "        # Use autograd to compute the backward pass. This call will compute the\n",
    "        # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "        # After this call model parameters '.grad' will be the Tensors holding the gradient\n",
    "        # of the loss with respect to w. Remember to zero out the gradients before each pass.\n",
    "        \n",
    "        pass  # zero the gradient buffers of the optimizer using .zero_grad()\n",
    "        output = None  # compute the model output (model() is equivalent to model.forward())\n",
    "        loss = None  # compute the loss between the model output and the labels using the loss function\n",
    "        pass  # compute the gradients of the loss using .backward()\n",
    "        pass  # Update the model weights via SGD (optimizer) using .step() \n",
    "        \n",
    "        # Note that you can also manually update weights \n",
    "        # for f in model.parameters():\n",
    "        #     f.data.sub_(f.grad.data * learning_rate)\n",
    "        \n",
    "        \n",
    "        # print the loss\n",
    "        if (i+1) % (6000//batch_size) == 0:\n",
    "            print(f'epoch: {epoch}   iter: {i+1}   batch_loss: {loss}')       \n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()  # sets the model into evaluation mode \n",
    "                  # (important if you use batch_norm, droupout and similar layers \n",
    "                  # that behave differently during training and during evaluation)\n",
    "            \n",
    "    with torch.no_grad(): # we don't need to track gradients during evaluation\n",
    "        for x, y in valid_dataloader:\n",
    "            x = None  # flatten 'x' and move it to device\n",
    "            y = None  # move 'y' to device\n",
    "            output = None  # compute the model output (model() is equivalent to model.forward())\n",
    "            \n",
    "            y_pred = None  # convert logits (model outputs) to class probabilities \n",
    "                           # (use torch.softmax or log_softmax)\n",
    "            \n",
    "            _, predicted = torch.max(y_pred, 1)  # find the most probable class (use torch.max)\n",
    "            \n",
    "            total+= y.size(0)\n",
    "            correct+= (predicted == y).sum()\n",
    "        validation_accuracy = 100 * float(correct)/total\n",
    "        print(f'epoch: {epoch}   validation accuracy: {validation_accuracy}%' )\n",
    "        \n",
    "        \n",
    "    # print epoch runtime\n",
    "    end_time = time.time()\n",
    "    print(f'--- Epoch completed in {end_time-start_time} seconds --- \\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "# you should see the loss decreasing during training\n",
    "# you should see accuracy increasing with every epoch\n",
    "# expect validation accuracy above 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer NN\n",
    "\n",
    "Now that you have familiarized yourself with the basic training pipeline, we will do some experiments with the model architecture\n",
    "\n",
    "model (neural network) \n",
    "- you can see the Logistic Regression model as a single-layer neural network, now try adding more layers ([torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear))\n",
    "- try different activation functions between the layers or at the output ([torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions))\n",
    "- how does changing the final activation affect the loss function?\n",
    "    \n",
    "As above, try playing with the following parameters and see if you can get better results or speed-up:\n",
    "- learning_rate (you can try different [optimizers](https://pytorch.org/docs/stable/optim.html) as well)\n",
    "- device (NOTE: you will need a gpu with cuda in order to play with this parameter)\n",
    "- batch_size\n",
    "- num_epochs\n",
    "- num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "MLN1"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# define your model class\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "# you should see no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "MLN2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# Go ahead and make your own training pipeline following the code outline defined above.\n",
    "\n",
    "# create the model\n",
    "model = None  # instance your new model\n",
    "\n",
    "# create the optimizer \n",
    "learning_rate = None\n",
    "optimizer = None  # use SGD as above, or try something else, e.g. Adam\n",
    "    \n",
    "# create the CrossEntropy Loss function\n",
    "loss_func = None\n",
    "\n",
    "\n",
    "# create transforms for data preprocessing and augmentations\n",
    "train_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])  # you might add some augmentations and normalization\n",
    "valid_test_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])  # define your validation/test transforms = usually just normalization\n",
    "\n",
    "# get some datasets\n",
    "train_dataset = torchvision.datasets.FashionMNIST('./data', train=True, transform=train_transforms, download=True)\n",
    "valid_dataset = torchvision.datasets.FashionMNIST('./data', train=True, transform=valid_test_transforms, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST('./data', train=False, transform=valid_test_transforms, download=True)\n",
    "\n",
    "# make dataloaders\n",
    "num_epochs = None\n",
    "batch_size = None\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_subset_sampler, num_workers=num_workers)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, sampler=valid_subset_sampler, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "# you should see no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "MLN3"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# Copy the code from TL2 cell and train your best model.\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "# you should see the loss decreasing during training\n",
    "# you should see accuracy increasing with every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Competition\n",
    "### Single fully connected model\n",
    "Your task is to use the framework you wrote above to train the best possible model with the following constraints:\n",
    "- single model only, no ensembles\n",
    "- no convolutional or pooling layers (you can use dropout and/or batch norm)\n",
    "- any activation functions\n",
    "- any loss, any optimizer\n",
    "- only 10 epochs allowed for training\n",
    "\n",
    "Note: Remember that you can add augmentations to your training dataset and normalization to all datasets :)\n",
    "\n",
    "Once you are satisfied with the achieved validation accuracy, continue below to compute the test accuracy.\n",
    "Some scores on the FashionMNIST are provided below, however, most of these were trained for hundreds of epochs, and with a lot of augmentations so don't get discouraged if you dont beat all of them :)\n",
    "- Human performance: 83.5%\n",
    "- Logistic regression: 83.9%\n",
    "- MLP (two layers): 87.4%\n",
    "- MLP (four layers): 88.3%\n",
    "- PSIML6 Competition Winner: 88.99\n",
    "- ConvNet (2 layers only): 91.9%\n",
    "- GoogLeNet: 93.7\n",
    "- PSIML6 Competition Winner (no-limit): 94.29\n",
    "- WideResNet with heay augmentations: 96.3% \n",
    "- Shake-Shake (SAM): 96.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# Calculate test accuracy similarly to how you calculated the validation set accuracy\n",
    "\n",
    "pass\n",
    "\n",
    "test_accuracy = None\n",
    "\n",
    "print(f'Final test accuracy: {test_accuracy}%' )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "# for example:\n",
    "Final test accuracy: 87.78%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
