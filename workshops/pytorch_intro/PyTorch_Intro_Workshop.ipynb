{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Intro Workshop\n",
    "### Introduction\n",
    "Welcome to the PSIML's PyTorch Intro Workshop. \n",
    "#### Goals\n",
    "In this workshop you will use some of the common functionalities of PyTorch to:\n",
    "1. replicate common NumPy functionalities,\n",
    "2. extend them using 'device' and 'AutoGrad'\n",
    "3. implement a simple dataloading pipeline.\n",
    "\n",
    "### Key Ingredients\n",
    "* Tensors (think of them as numpy arrays and more!)\n",
    "    *  creating tensors\n",
    "    *  tensor operators\n",
    "    *  indexing and slicing\n",
    "    *  view instead of reshape\n",
    "    *  device (\"cpu\", \"cuda\")\n",
    "    *  autograd\n",
    "* Datasets and Dataloaders (data wrappers that facilitate data manipulation, modularity and parallelism)\n",
    "    * map style dataset: implement '\\_\\_len\\_\\_' and '\\_\\_getitem\\_\\_'\n",
    "        * or iterator style dataset: implement '\\_\\_iter\\_\\_'\n",
    "    * sampling and batching\n",
    "    * simple parallelism using 'num_workers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "I1"
    ]
   },
   "outputs": [],
   "source": [
    "# Some useful imports\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "When unsure, always check the docs:\n",
    "https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT1"
    ]
   },
   "outputs": [],
   "source": [
    "# PyTorch Tensor is in many ways equivalent to numpy NDArrays\n",
    "# Let's create some tensors\n",
    "e1 = torch.Tensor()  # an empty tensor\n",
    "print(e1)\n",
    "\n",
    "e2 = torch.empty(5, 3)  # allocates memory but the values are undefined\n",
    "print(e2)\n",
    "\n",
    "x = torch.ones(3,4)  # a tensor of ones\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "x_len = None  # get the lenght (size of first dimension) of tensor x\n",
    "print(x_len)\n",
    "\n",
    "x_shape = None  # get the shape/size of tensor x (.shape is the alias for .size(), you can use either)\n",
    "x_size = None   # note that in pytorch .size() has different meaning than .size in numpy\n",
    "print(x_shape)\n",
    "print(x_size)\n",
    "\n",
    "x_ndim = None  # get the dimensionality of tensor x\n",
    "print(x_ndim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "3\n",
    "torch.Size([3, 4])\n",
    "torch.Size([3, 4])\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT3"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "t1 = None  # a tensor of ones with shape (4,4)\n",
    "print(t1)\n",
    "\n",
    "t2 = None  # a tensor of zeros with shape (1,2,3)\n",
    "print(t2)\n",
    "\n",
    "torch.manual_seed(0)  # random generator seed - for reproducibility set the seed to 0\n",
    "t3 = None  # a tensor of numbers from uniform [0,1) distribution with shape (5,2)\n",
    "print(t3)\n",
    "\n",
    "torch.manual_seed(0)  # random generator seed - for reproducibility set the seed to 0\n",
    "t4 = None  # a tensor of numbers from standard normal distribution with shape (10,3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1.]])\n",
    "tensor([[[0., 0., 0.],\n",
    "         [0., 0., 0.]]])\n",
    "tensor([[0.4963, 0.7682],\n",
    "        [0.0885, 0.1320],\n",
    "        [0.3074, 0.6341],\n",
    "        [0.4901, 0.8964],\n",
    "        [0.4556, 0.6323]])\n",
    "tensor([[-1.1258, -1.1524, -0.2506],\n",
    "        [-0.4339,  0.8487,  0.6920],\n",
    "        [-0.3160, -2.1152,  0.3223],\n",
    "        [-1.2633,  0.3500,  0.3081],\n",
    "        [ 0.1198,  1.2377, -0.1435],\n",
    "        [-0.1116, -0.6136,  0.0316],\n",
    "        [-0.4927,  0.2484,  0.4397],\n",
    "        [ 0.1124, -0.8411, -2.3160],\n",
    "        [-0.1023,  0.7924, -0.2897],\n",
    "        [ 0.0525,  0.5229,  2.3022]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT4"
    ]
   },
   "outputs": [],
   "source": [
    "# You can also create tensors by converting the already familiar numpy arrays or even lists to tensors\n",
    "list_array = [[0.1, 0.2, 0.3],[1, 2, 3]]\n",
    "print(list_array)\n",
    "\n",
    "np_array = np.asarray(list_array)\n",
    "print(np_array)\n",
    "\n",
    "torch_tensor_from_list = torch.tensor(list_array)\n",
    "print(torch_tensor_from_list)\n",
    "\n",
    "torch_tensor_from_np = torch.from_numpy(np_array)  # note that you can also do torch.tensor(np_array)\n",
    "print(torch_tensor_from_np)\n",
    "\n",
    "# If you want to convert back to numpy\n",
    "np_array_from_torch = torch_tensor_from_np.numpy()  # easy-peasy :)\n",
    "print(np_array_from_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT5"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "np.random.seed(0)  # random generator seed - for reproducibility set the seed to 0\n",
    "n1 = None  # a numpy array of random integers from [0,10] with shape (5,5)\n",
    "print(n1)\n",
    "\n",
    "t1 = None  # convert n1 to torch.tensor\n",
    "print(t1)\n",
    "\n",
    "n2 = None  # convert t1 back to numpy array\n",
    "print(n2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "[[ 5  0  3  3  7]\n",
    " [ 9  3  5  2  4]\n",
    " [ 7  6  8  8 10]\n",
    " [ 1  6  7  7  8]\n",
    " [ 1  5  9  8  9]]\n",
    "tensor([[ 5,  0,  3,  3,  7],\n",
    "        [ 9,  3,  5,  2,  4],\n",
    "        [ 7,  6,  8,  8, 10],\n",
    "        [ 1,  6,  7,  7,  8],\n",
    "        [ 1,  5,  9,  8,  9]], dtype=torch.int32)\n",
    "[[ 5  0  3  3  7]\n",
    " [ 9  3  5  2  4]\n",
    " [ 7  6  8  8 10]\n",
    " [ 1  6  7  7  8]\n",
    " [ 1  5  9  8  9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT6"
    ]
   },
   "outputs": [],
   "source": [
    "# You can set the type for the tensor data at creation time or change them later using 'dtype'\n",
    "x_int = torch.tensor([1,2,3], dtype=torch.int32)\n",
    "print(x_int)\n",
    "\n",
    "x_float = x_int.to(dtype=torch.float64)\n",
    "print(x_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT7"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "a1 = [[0.1, 1.2, 2.3],[3.4, 4.5, 5.6]]\n",
    "print(a1)\n",
    "\n",
    "t1 = None  # convert a1 to torch.tensor\n",
    "print(t1)\n",
    "\n",
    "t2 = None  # change the type of t1 to 16-bit integer\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "[[0.1, 1.2, 2.3], [3.4, 4.5, 5.6]]\n",
    "tensor([[0.1000, 1.2000, 2.3000],\n",
    "        [3.4000, 4.5000, 5.6000]])\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5]], dtype=torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT8"
    ]
   },
   "outputs": [],
   "source": [
    "# You can also create a tensor based on an existing tensor using new_* methods (they take size as argument)\n",
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "\n",
    "y = x.new_ones(x.size())  # a tensor of ones with the size and type of x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "CT9"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "z = None  # a tensor of zeros with the size and type of x\n",
    "print(z)\n",
    "\n",
    "z1 = None  # a tensor of ones with the type of x and size 1 x 2nd-dimenson-of-x\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[0, 0, 0],\n",
    "        [0, 0, 0]])\n",
    "tensor([[1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "Exactly the same as in numpy.\n",
    "\n",
    "When unsure, always check the docs:\n",
    "https://numpy.org/doc/1.18/reference/arrays.indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "IS1"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's start with a tensor x containing a representation for a batch of 4 2x2 RGB images\n",
    "batch_size = 4\n",
    "channels = 3  # lets say that the channels are in order of (Red, Green, Blue)\n",
    "height = 2\n",
    "width = 2\n",
    "\n",
    "torch.manual_seed(0)  # random generator seed - for reproducibility set the seed to 0\n",
    "x = torch.randint(low=0, high=256, size=(batch_size, channels, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "IS2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "x_first = None  # first image\n",
    "print (x_first)\n",
    "\n",
    "x_last_r = None  # only the red channel of the last image\n",
    "print(x_last_r)\n",
    "\n",
    "x_1_1_blue = None  # first blue pixel of every image in the batch\n",
    "print(x_1_1_blue)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[[172,  47],\n",
    "         [117, 192]],\n",
    "\n",
    "        [[ 67, 251],\n",
    "         [195, 103]],\n",
    "\n",
    "        [[  9, 211],\n",
    "         [ 21, 242]]])\n",
    "tensor([[254,  79],\n",
    "        [175, 192]])\n",
    "tensor([  9, 230, 115, 243])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and Reshape\n",
    "Mostly similar to numpy.reshape() with some quirks.\n",
    "\n",
    "When unsure, always check the docs:\n",
    "https://pytorch.org/docs/stable/tensor_view.html#tensor-view-doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "VR1"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's start again with a tensor containing a representation for a batch of 4 images\n",
    "batch_size = 4\n",
    "channels = 3  # lets say that the channels are in order of (Red, Green, Blue)\n",
    "height = 2\n",
    "width = 2\n",
    "\n",
    "torch.manual_seed(0)  # random generator seed - for reproducibility set the seed to 0\n",
    "x = torch.randint(low=0, high=256, size=(batch_size, channels, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "VR2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "x_flattened = None  # replace the height, width and channel dimensions with a single dimension \n",
    "                    # -> (batch_size, channels*height*width)\n",
    "print(x_flattened)\n",
    "\n",
    "x_channels = None  # starting from x_flattened add the channels dimension back\n",
    "                   # -> (batch_size, channels, height*width)\n",
    "print(x_channels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[172,  47, 117, 192,  67, 251, 195, 103,   9, 211,  21, 242],\n",
    "        [ 36,  87,  70, 216,  88, 140,  58, 193, 230,  39,  87, 174],\n",
    "        [ 88,  81, 165,  25,  77,  72,   9, 148, 115, 208, 243, 197],\n",
    "        [254,  79, 175, 192,  82,  99, 216, 177, 243,  29, 147, 147]])\n",
    "tensor([[[172,  47, 117, 192],\n",
    "         [ 67, 251, 195, 103],\n",
    "         [  9, 211,  21, 242]],\n",
    "\n",
    "        [[ 36,  87,  70, 216],\n",
    "         [ 88, 140,  58, 193],\n",
    "         [230,  39,  87, 174]],\n",
    "\n",
    "        [[ 88,  81, 165,  25],\n",
    "         [ 77,  72,   9, 148],\n",
    "         [115, 208, 243, 197]],\n",
    "\n",
    "        [[254,  79, 175, 192],\n",
    "         [ 82,  99, 216, 177],\n",
    "         [243,  29, 147, 147]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device (NOTE: you will need a gpu with cuda in order to complete this cell)\n",
    "This is where we start seeing the usefulness of PyTorch. Move your data and variables to the GPU and exploit its computational capabilities to speed up your deep learning dozens of times. :)\n",
    "\n",
    "When unsure, always check the docs: https://pytorch.org/docs/stable/cuda.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "D1"
    ]
   },
   "outputs": [],
   "source": [
    "# Lets make some tensors again and see how we can move our data and computations to the GPU\n",
    "\n",
    "x = torch.tensor([[0.5, 0.2], [0.2, 0.5]])\n",
    "print(x)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('I have', torch.cuda.device_count(), 'GPU(s)')\n",
    "print('Current GPU ID:', torch.cuda.current_device())  # if you have more GPUs you can change the current\n",
    "                                                       # device context with torch.cuda.device(device)\n",
    "print('Current GPU Name:', torch.cuda.get_device_name(), '\\n')\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object (usually your GPU:0, but you can change that)\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings .to(\"cuda\")\n",
    "    z = x + y                              # computation happens on GPU since both x and y are in GPU memory\n",
    "    print(z)                               # result of the computation z is also on GPU. Bring it back to RAM using .to()\n",
    "    print(z.to(\"cpu\", torch.double))       # .to() can also change the dtype as before!    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "tensor([[0.5000, 0.2000],\n",
    "        [0.2000, 0.5000]])\n",
    "CUDA available: True\n",
    "I have 1 GPU(s)\n",
    "Current GPU ID: 0\n",
    "Current GPU Name: GeForce GTX 1060 \n",
    "\n",
    "tensor([[1.5000, 1.2000],\n",
    "        [1.2000, 1.5000]], device='cuda:0')\n",
    "tensor([[1.5000, 1.2000],\n",
    "        [1.2000, 1.5000]], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "D2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "device = None  # set the device to use your gpu\n",
    "\n",
    "torch.manual_seed(0)  # random generator seed - for reproducibility set the seed to 0\n",
    "a = None  # a tensor of numbers from uniform distribution with size (10, 4)\n",
    "print(a)\n",
    "b = None  # a tensor of ones with size (4,1) with the same data type as a\n",
    "print(b)\n",
    "\n",
    "# move a and b to gpu\n",
    "a_gpu = None\n",
    "print(a_gpu)\n",
    "b_gpu = None\n",
    "print(b_gpu)\n",
    "\n",
    "c_gpu = None # multiply a_gpu and b_gpu via matrix multiplication and confirm that your coutput is on gpu\n",
    "print(c_gpu)\n",
    "\n",
    "c_cpu = None # move the variable c_gpu to cpu and confirm that it is not on gpu anymore\n",
    "print(c_cpu.device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[0.4963, 0.7682, 0.0885, 0.1320],\n",
    "        [0.3074, 0.6341, 0.4901, 0.8964],\n",
    "        [0.4556, 0.6323, 0.3489, 0.4017],\n",
    "        [0.0223, 0.1689, 0.2939, 0.5185],\n",
    "        [0.6977, 0.8000, 0.1610, 0.2823],\n",
    "        [0.6816, 0.9152, 0.3971, 0.8742],\n",
    "        [0.4194, 0.5529, 0.9527, 0.0362],\n",
    "        [0.1852, 0.3734, 0.3051, 0.9320],\n",
    "        [0.1759, 0.2698, 0.1507, 0.0317],\n",
    "        [0.2081, 0.9298, 0.7231, 0.7423]])\n",
    "tensor([[1.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [1.]])\n",
    "tensor([[0.4963, 0.7682, 0.0885, 0.1320],\n",
    "        [0.3074, 0.6341, 0.4901, 0.8964],\n",
    "        [0.4556, 0.6323, 0.3489, 0.4017],\n",
    "        [0.0223, 0.1689, 0.2939, 0.5185],\n",
    "        [0.6977, 0.8000, 0.1610, 0.2823],\n",
    "        [0.6816, 0.9152, 0.3971, 0.8742],\n",
    "        [0.4194, 0.5529, 0.9527, 0.0362],\n",
    "        [0.1852, 0.3734, 0.3051, 0.9320],\n",
    "        [0.1759, 0.2698, 0.1507, 0.0317],\n",
    "        [0.2081, 0.9298, 0.7231, 0.7423]], device='cuda:0')\n",
    "tensor([[1.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [1.]], device='cuda:0')\n",
    "tensor([[1.4850],\n",
    "        [2.3280],\n",
    "        [1.8385],\n",
    "        [1.0036],\n",
    "        [1.9410],\n",
    "        [2.8681],\n",
    "        [1.9612],\n",
    "        [1.7957],\n",
    "        [0.6281],\n",
    "        [2.6034]], device='cuda:0')\n",
    "cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad\n",
    "Another very very useful functionality of PyTorch is the AutoGrad. As the name suggests, it allows you to automatically compute the gradients of variables in your graph. Let's see some examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "AG1"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's create some tensors and set requires_grad to True to enable AutoGrad (automatic gradient computation)\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad)  # None - because we havent done any computations yet\n",
    "\n",
    "# Now let's do some calculations and the get our gradients \n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "z = y * y * 3 \n",
    "out = z.mean() \n",
    "print(z, out)\n",
    "\n",
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "pass # compute the gradients of 'out' tensor using the .backward() \n",
    "     # AutoGrad will then compute partial derivatives of out with respect to every tensor \n",
    "     # that is involved in the computatinal graph of 'out' and also has 'requires_grad=True'.\n",
    "     # You then can access gradients of out with respect to such tensor x using 'x.grad'\n",
    "     # For convenience here is the analytic solution for gradient of 'out' with respect to 'x'\n",
    "     # out = 1/4 * sum(3*(x_i+2)^2) | dout/dx_i = 3/2*(x_i+2) = 4.5\n",
    "x_grad = None  # get the gradients of 'x'\n",
    "print(x_grad)\n",
    "\n",
    "# Note that tensor.backward() is a shortcut for tensor.backward(torch.Tensor([1])). \n",
    "# Therefore, it is only valid if the said tensor contains a single element."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]], requires_grad=True)\n",
    "None\n",
    "tensor([[3., 3.],\n",
    "        [3., 3.]], grad_fn=<AddBackward0>)\n",
    "<AddBackward0 object at 0x0000017C710AD8C8>\n",
    "tensor([[27., 27.],\n",
    "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
    "tensor([[4.5000, 4.5000],\n",
    "        [4.5000, 4.5000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "AG2"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "# let's try some more computations with x\n",
    "y = x + 5\n",
    "z = x * y * 3 \n",
    "out2 = z.sum() \n",
    "\n",
    "pass\n",
    "\n",
    "pass  # compute the gradients of 'out2' tensor\n",
    "      # For convenience here is the analytic solution for gradient of 'out2' with respect to 'x'\n",
    "      # out = sum(3*x_i*(x_i+5)) | dout/dx_i = 3*(2*x_i+5) = 21\n",
    "\n",
    "x_grad = None  # get the gradients of 'x'\n",
    "print(x_grad)\n",
    "\n",
    "# Remember to zero out the gradients before every '.backward()' call or AutoGrad \n",
    "# will keep accumulating the gradients\n",
    "\n",
    "# Note: If this still looks tedious, do not worry, this is just a brief look under the hood and\n",
    "# most of it is automated by higher level classes and functions when you actually train a model. :)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[21., 21.],\n",
    "        [21., 21.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader\n",
    "PyTorch datasets present a nice functionality to handle data loading and preprocessing as well as being the segue to PyTorch Dataloaders that handle sampling, batching and parallelism.\n",
    "\n",
    "When unsure, alsways check the docs: https://pytorch.org/docs/stable/data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DD1"
    ]
   },
   "outputs": [],
   "source": [
    "# Some useful imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DD2"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's create some data and calculate some basic statistics\n",
    "# Data: 10 2-dimensional integer vectors from 0 to 100\n",
    "np.random.seed(seed=0)  # random generator seed - for reproducibility set the seed to 0\n",
    "data = pd.DataFrame(np.random.randint(low=0, high=100+1, size=(10,2)), columns = ['x','y'])\n",
    "sb.scatterplot(x=\"x\", y=\"y\", data=data)\n",
    "mean = data.mean().to_numpy()\n",
    "print(mean)\n",
    "std = data.std().to_numpy()\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DD3"
    ]
   },
   "outputs": [],
   "source": [
    "# So, let's create our own simple pytorch Dataset to see how it all works out\n",
    "# A map style dataset needs __len__() and __getitem__() methods overwritten \n",
    "\n",
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Wraps a DataFrame and adds some transforms\"\"\"\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame): pandas DataFrame object\n",
    "            transform (callable, optional): transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        pass  # return the length of your dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = None # get a row of 'self.data' at index 'idx' (use .iloc) as a numpy array \n",
    "        sample = None # convert the numpy array to pytorch tensor with type 'double'\n",
    "        if self.transform:\n",
    "            # This is where we apply some preprocessing transformations and/or augmentations to our data\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample # finaly we return one sample from our dataset at given index\n",
    "    \n",
    "# Let's now define a very simple normalization transform: x = (x-mean)/std\n",
    "class NormalizationTransform(object):\n",
    "    # First we need to initialize the transform object with the parameters necessary to perform the transformation\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        pass  # return the required normalization transform"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "# you should see no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DD4"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "# let's create the normalization transformation for our simple data of 2 dimensional vectors (defined above)\n",
    "normalization_transform = NormalizationTransform(mean, std)\n",
    "\n",
    "# Now let's instance our dataset providing the normalization transform to it \n",
    "# and then get we will get some samples from it using the pytorch dataloader\n",
    "our_dataset = SimpleDataset(data=data, transform=normalization_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "DD5"
    ]
   },
   "outputs": [],
   "source": [
    "# Here is the dataloader with the default parameters. You can run the following code as is.\n",
    "# Then, you should try increasing the batch size to 5 and turning the shuffling on.\n",
    "our_dataloader = torch.utils.data.DataLoader(dataset=our_dataset,\n",
    "                                             batch_size=1, shuffle=False, sampler=None,\n",
    "                                             batch_sampler=None, num_workers=0,\n",
    "                                             pin_memory=False, drop_last=False, timeout=0,\n",
    "                                             worker_init_fn=None)\n",
    "# This is basically one full epoch:\n",
    "for batch in our_dataloader:\n",
    "    # Do something with your batch\n",
    "    batch = batch.to(device)  # we generally want to do our training on the GPU \n",
    "                              # (remember that we set our device to torch.device(\"cuda\") earlier)\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[-0.8564, -0.3091]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[0.2486, 0.3030]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[ 0.4144, -1.4719]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[ 1.2983, -1.1047]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[-1.2983,  0.9150]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[0.5801, 0.9456]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[ 1.5746, -1.3801]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[-0.0829,  0.2417]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[-1.1326,  0.9150]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[-0.7459,  0.9456]], device='cuda:0', dtype=torch.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a deeper look at some of the parameters of the DataLoader to see\n",
    "how they can be useful to us. Specifically we will focus on most common scenarios \n",
    "and give hints for the less common ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "torch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. \n",
    "* when sampler and batched_sampler is None a Sampler will be constructed automatically based on the shuffle parameter\n",
    "    * shuffle=False - data is sampled in order they are in the dataset\n",
    "    * shuffle=True - indices of the dataset (from 0 to len(dataset)-1) are shuffled and then sampled sequentially\n",
    "* if more control over sampling is needed you can specify a sampler object to \n",
    "    * decide if the sampling is done with or without replacement\n",
    "    * specify a distribution over samples in case of non-uniform (weighted) sampling\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching \n",
    "\n",
    "If you tried increasing the batch more than 1 you saw that the dataloader returns a 'batched' tensor\n",
    "for each batch. This means that the 'batch_size' samples will be taken from our dataset and \n",
    "grouped together into a single tensor which improves computation speed of the models we pass this batch to.\n",
    "This is called automatic batching and what it does is basically concatenating the samples of a batch into\n",
    "a pytorch tensors. That in turn requires the sample tensors to be of same size (which is often true). \n",
    "\n",
    "In case you need a custom made batch you can write a 'collate_fn' that does more than just \n",
    "concatenating the sample tensors. E.g. filtering bad samples, padding/trimming sequences to \n",
    "fit desired size, etc. \n",
    "\n",
    "If the size of your dataset is not divisible by your 'batch_size', the last batch of your dataloader will\n",
    "not be full. In case your model requires a full batch, you can simply discard the last batch by setting the\n",
    "'drop_last' parameter to True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism\n",
    "With default parameters, the dataloader will spawn a single process that loads your data. In case that the dataloading is a bottleneck of your system a simple and efficient speedup can be achieved by increasing the num_workers parameter. Instead of one, the dataloader will now spawn num_workers processes and each process will be collecting a single batch of data and returning it once it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "SBP1"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's create another dataset that will have a problem of imballanced classes.\n",
    "# Then we will use a weighted sampler to fix the problem.\n",
    "\n",
    "np.random.seed(seed=0)  # random generator seed - for reproducibility set the seed to 0\n",
    "class0_data = pd.DataFrame(np.random.randint(low=0, high=50+1, size=(10,2)), columns = ['x','y'])\n",
    "class1_data = pd.DataFrame(np.random.randint(low=50, high=100+1, size=(4,2)), columns = ['x','y'])\n",
    "class0_data['class'] = 0\n",
    "class1_data['class'] = 1\n",
    "data = pd.concat([class0_data, class1_data])\n",
    "sb.scatterplot(x=\"x\", y=\"y\", data=data, hue=data['class'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "SBP2"
    ]
   },
   "outputs": [],
   "source": [
    "# create a dataset (this time we will skip creating the transforms)\n",
    "our_dataset = Simple_Dataset(data=data, transform=None)\n",
    "\n",
    "### COMPLETE THE CODE BELOW\n",
    "weights = None  # calculate the weights for each sample to solve the imbalance problem as 1/classX_num_samples\n",
    "print(weights)\n",
    "\n",
    "dataset_size = None  # set the desired dataset size, e.g. 20\n",
    "\n",
    "weighted_sampler = None  # instance a weighted sampler (torch.utils.data.WeightedRandomSampler), \n",
    "                         # set the num_samples to dataset_size and replacement to True if your dataset_size variable \n",
    "                         # is larger than your actual dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "[0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.25 0.25 0.25 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "SBP3"
    ]
   },
   "outputs": [],
   "source": [
    "### COMPLETE THE CODE BELOW\n",
    "\n",
    "# Here is the dataloader with our weighted_sampler. You can run the following code as is.\n",
    "# Note that you won't be using the shuffle parameter anymore as setting the sampler overrules it.\n",
    "\n",
    "# Try changing the following parameters along with the parameters of your weighted_sampler \n",
    "# and try to predict what will happen:\n",
    "# - batch_size\n",
    "# - drop_last\n",
    "# - num_workers\n",
    "our_dataloader = torch.utils.data.DataLoader(dataset=our_dataset,\n",
    "                                             batch_size=1, shuffle=None, sampler=weighted_sampler,\n",
    "                                             batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                                             pin_memory=False, drop_last=False, timeout=0,\n",
    "                                             worker_init_fn=None)\n",
    "# This is basically one full epoch:\n",
    "for batch in our_dataloader:\n",
    "    # Do something with your batch\n",
    "    batch = batch.to(device)  # we generally want to do our training on the GPU \n",
    "                              # (remember that we set our device to torch.device(\"cuda\") earlier)\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "tensor([[ 3., 39.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[96., 74.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[21., 50.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[96., 74.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[96., 74.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[67., 87.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[67., 87.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[21., 50.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[75., 63.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[67., 87.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[ 9., 19.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[ 9., 19.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[39., 23.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[58., 59.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[96., 74.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[39., 23.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[75., 63.,  1.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[24., 12.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[36., 23.,  0.]], device='cuda:0', dtype=torch.float64)\n",
    "tensor([[44., 47.,  0.]], device='cuda:0', dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
