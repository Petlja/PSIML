{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e5eef3",
   "metadata": {},
   "source": [
    "## Machine Learning Summer School 2021 - Reinforcement Learning Workshop\n",
    "# Assignment 2: [Deep Q Learning](https://arxiv.org/abs/1312.5602)\n",
    "\n",
    "Welcome to DQN part of the workshop! :) Here you will be tasked with implementing the core logic behind the [famous paper by Mnih et al.](https://arxiv.org/abs/1312.5602) that has brought RL into the deep learning era. This notebook is divided into three main sections: \n",
    "\n",
    "1. **Problem description**: Where we introduce the Cartopole problem which we'll solve using the DQN paper.\n",
    "2. **DQN implementation**: Where we explain our implementation of the DQN paper, and task you with comleting that implementation in two exercises.\n",
    "3. **Experimentation**: Where we give notes on how to tune the DQN algorithm hyper-parameters and ask you to analyze results; and provide some closure notes.\n",
    "\n",
    "**NOTE!**: Certain portion of the code is hidden, and only imported into this notebook. You can see full code base by clicking the Jupyter logo in the upper left corner.\n",
    "\n",
    "## Problem description\n",
    "\n",
    "We will test DQN on [The Cartpole Problem](https://gym.openai.com/envs/CartPole-v1/), where the pole is attached by an un-actuated joint to a cart which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the chart. The pendulum start upright, and the goal is to prevent it from falling over. A reward of +1 is provided from every tilmestep that the pole remains upright. The episode ends when the pole is more then 15 degrees from vertical, or the cart moves more then 2.4 from the center. At every time step, the agent observes its position, velocity, angle and angular velocity. These are the observable states of this world. At any state, the cart only has two possible actions: move to the left (0) and move to the right (+1). In other words, the state-space has four dimensions of continues values and the action space has one dimension of two discrete values.\n",
    "\n",
    "Video below shows an **untrained agent** trying to solve this problem. Since it has not yet learned how to keep the pole upright, episodes quickly terminate when the pole surpasses the 15 degrees allowed limit, as described above.\n",
    "\n",
    "![cartpole](resources/cartpole_env.mp4)\n",
    "\n",
    "If the video above won't load properly, run the cell below to display the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fae876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Video('./resources/cartpole_env.mp4', html_attributes=\"loop autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1cde1",
   "metadata": {},
   "source": [
    "## DQN implementation\n",
    "\n",
    "### Intro\n",
    "\n",
    "[DQN](https://arxiv.org/abs/1312.5602) is the deep learning variant of the standard Q-Learning algorithm. Full pseudo-code for DQN is provided below:\n",
    "\n",
    "1. Initialize replay memory (buffer) $D$ to capacity $N$\n",
    "2. Initialize *live* action-value function neural network $Q$ with random weights.\n",
    "3. Initialize *target* action-value function neural network $\\hat{Q}$ by copying weights from the *live* one. \n",
    "4. **for** episode = $1$, $M$ **do**\n",
    "    1. Restart episode and initialize start state $s_1$\n",
    "    2. **for** t = $1$, $T$ **do**\n",
    "        1. With probability $\\varepsilon$ select random action $a_t$, otherwise <font color=\"green\"> select $a_t = \\max_aQ(s_t, a_t; \\theta)$ </font>\n",
    "        2. Execute selected action $a_t$ in the environment, and get $s_{t+1}$, $r_t$ and $\\eta_{t+1}$, where $\\eta_{t+1}$ indicates whether $s_{t+1}$ is a terminal state.\n",
    "        3. Store *transition* $(s_j, a_j, s_{j+1}, r_j, \\eta_{j+1})$ in replay memory (buffer) $D$.\n",
    "        4. Sample mini-batch of transitions of size $h$ from replay memory $D$.\n",
    "        5. <font color=\"green\"> Calculate target value $y_j$ using the following rule: $y_j = r_j + (1 - \\eta_{j+1})\\gamma\\max_{a'}\\hat{Q}(s_{j+1}, a; \\theta))$ </font>\n",
    "        6. Calculate temoral difference error *TD Error*: $\\delta = y_j - Q(s_j, a_j; \\theta)$\n",
    "        7. Regress the *live* network $Q$ with respect to $(\\delta)^2$\n",
    "    4. **end for**\n",
    "5. **end for**\n",
    "\n",
    "We would like to draw attention to the similarity of the DQN algorithm to standard Q-learning. In fact, if we were to use tabular representations of Q values, and remove the concepts of the *target* network and replay memory, the pseudo-code above would describe Q-learning instead. These additional concepts, however, are precisely the reason of DQN's success, and are the main focus of this exercise. <font color=\"green\"> Green text specifies pseudo-code parts you are tasked with implementing. </font>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "DQN implementation is divided into five cells. We encourage you to read through all cells, but to only modify those specified as **Exercise 1: Action selection** and **Exercise 2: Q-value update**. Throughout the rest of this section we explain the code and the reasoning behind it, and leave the exercises for you to complete :)\n",
    "\n",
    "**NOTE!**: All cells need to be ran!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090e569",
   "metadata": {},
   "source": [
    "Run the cell below to import the required libraries. We use [PyTorch](https://pytorch.org) to implement the DQN algorithm, while [Gym](https://gym.openai.com) provides the Cartpole environment. File *nets.py* implements the neural networks that will be used in this tutorilal, while *utils.py* provides the neccessary utilities. You can see their contents (and full project structure in general) by clicking the Jupyter logo in the top left corner of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557c9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from env_wrapper import EnvWrapper\n",
    "from nets import DQN\n",
    "from utils import create_run_name, visualize_result\n",
    "from rb import ReplayBuffer\n",
    "\n",
    "\n",
    "import jdc\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "STORE_PATH = './tmp_deep_q_learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c186b94",
   "metadata": {},
   "source": [
    "The next couple of cells implement the DQN agent we will use to solve the cartpole problem. In the cell below we define the DQN initialization procedure. **state_size**, **action_size**, **num_hidden**, **hidden_units** are used to construct the DQN neural networks the agent will use. **state_size** will define their input dimension, **action_size** will specify their output dimension, while **num_hidden** and **hidden_units** specify the number of hidden layers and the number of units in each, respectively.\n",
    "\n",
    "The rest of the constructor arguments are some of the hyper-parameters of the algorithm you will be able to play around with. **gamma** ($\\gamma$) represents the discount factor, **batch_size** represents the number of samples in mini-batch $h$, while **lr** is the learning rate of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe37e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_size, action_size, gamma=0.95, batch_size=256, lr=0.00025, num_hidden=2,\n",
    "                 hidden_units=64):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.gamma = gamma\n",
    "        self.name = 'DQN'\n",
    "\n",
    "        # We create \"live\" and \"target\" networks from the original paper.\n",
    "        self.current = DQN(state_size, action_size, h=hidden_units, num_hidden=num_hidden)\n",
    "        self.target = DQN(state_size, action_size, h=hidden_units, num_hidden=num_hidden)\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.update_target_model()\n",
    "\n",
    "        # Replay buffer (memory) initialization.\n",
    "        self.rb = self.init_rb()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Learning rate and optimizer used to update the \"live\" network in DQN.\n",
    "        learning_rate = lr\n",
    "        self.optimizer = torch.optim.Adam(self.current.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712623",
   "metadata": {},
   "source": [
    "The next cell specifies DQNAgent methods that deal with the replay memory and experience replay. **init_rb(self)** constructs the replay buffer $D$ which is used to store transitions. **remember(self, state, action, reward, next_state, done)** stores the transition $(s_j, a_j, s_{j+1}, r_j, \\eta_{j+1})$ to the replay buffer, while **sample(self)** samples minibatch of size DQNAgent.batch_size ($h$) from it. If you want to check the specifics of our implementation of replay buffer, you can find it in [rb.py](../edit/rb.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657da6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def init_rb(self):\n",
    "    # Replay buffer initialization.\n",
    "    replay_buffer = ReplayBuffer(1e5, obs_dtype=np.float32, act_dtype=np.int64, default_dtype=np.float32)\n",
    "    return replay_buffer\n",
    "\n",
    "def remember(self, state, action, reward, next_state, done):\n",
    "    # Remember (Q,S,A,R,S') as well as whether S' is terminating state.\n",
    "    self.rb.add(obs=state, act=action, rew=reward, next_obs=next_state, done=done)\n",
    "\n",
    "def sample(self):\n",
    "    states, actions, next_states, rewards, dones = self.rb.sample(self.batch_size)\n",
    "    one_hot_acts = torch.squeeze(\n",
    "        torch.nn.functional.one_hot(actions, num_classes=self.action_size))\n",
    "    return states, one_hot_acts, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46004d",
   "metadata": {},
   "source": [
    "DQN paper introduces the *target* network $\\hat{Q}$ which is used to estimate optimal Q-values when calculating the TD error $\\delta$. Target network differs from the *live* one in that it does not update its parameters $\\theta$ as frequently. This produces more stable Q-value estimates, and is aimed at stabilizing the training process by giving the *live* network necessary time to converge across multiple mini-batches, before the *target* network is finally updated. \n",
    "\n",
    "There are multiple ways we can orchestrate theese periodic updates. The original DQN paper simply copies the weights of the *live* network every $K$ episodes to the *traget* network, which we adopt here as well and implement in **update_target_model(self)** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cbc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def update_target_model(self):\n",
    "    self.target.load_state_dict(self.current.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ddf42",
   "metadata": {},
   "source": [
    "#### Exercise 1: Action selection\n",
    "\n",
    "Complete the function **select_action(self, state)** below to greedily select best actions using the current estimate of the Q-values. **We strongly encourage you to read through the rest of the code explanations before starting the exercise.** This exercise corresponds to <font color=\"green\"> pseudo-code algorithm step 4.B.a</font>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4676aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def select_action(self, state):\n",
    "    # ######## TODO: IMPLEMENT DQN ACTION SELECTION HERE ########\n",
    "    # Hint 1: DQN uses the same greedy policy as regular Q-Learning\n",
    "    # Hint 2: We want to select purely greedy action here, random action selection\n",
    "    # based on epsilon is already handled\n",
    "    action = None\n",
    "\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce86916",
   "metadata": {},
   "source": [
    "#### Exercise 2: Q-value update\n",
    "\n",
    "Complete the function **backward(self)** below to update current estimates of Q-values. **We strongly encourage you to read through the rest of the code explanations before starting the exercise.** This exercise corresponds to pseudo-code algorithm step <font color=\"green\"> pseudo-code algorithm step 4.B.e</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b3b05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def backward(self):\n",
    "    # ######## TODO: IMPLEMENT DQN UPDATE STEP HERE ########\n",
    "    # # 1. Sample mini-batch of stored transitions from the replay buffer\n",
    "\n",
    "\n",
    "\n",
    "    # # 2. Implement the learning part of the DQN algorithm.\n",
    "    # ### a. Use \"states\" (batch_dim x self.state_size) and \"actions\" (batch_dim x action_size)\n",
    "    #     to get Q values (batch_dim x 1) of actions agent had played out in the environment.\n",
    "    #     USE qs_selected VARIABLE TO STORE RESULT!\n",
    "    qs_selected = None\n",
    "\n",
    "\n",
    "\n",
    "    # ### b. Use \"next_states\" (batch_dim x self.state_size) to calculate the target value\n",
    "    #     Q values obtained in a. should be regressed to.\n",
    "    #     Hint 1: Target network plays important part here!\n",
    "    #     Hint 2: Pay attention to PyTorch gradient accumulation!\n",
    "    #     Hint 3: You can use \"dones\" (batch_dim x 1) to check whether \"next_states\" are\n",
    "    #     terminating!\n",
    "    #     USE qs_target VARIABLE TO STORE RESULT!\n",
    "    qs_target = None\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Code below updates the \"live\" network self.current using the variables you have\n",
    "    # calculated in TODO section above: qs_selected and qs_target. DO NOT MODIFY THIS CODE.\n",
    "\n",
    "    # We calculate the absolute difference between current and target values q values,\n",
    "    # which is useful info for debugging.\n",
    "    with torch.no_grad():\n",
    "        td_error = torch.abs(qs_target - qs_selected)\n",
    "\n",
    "    # We update the \"live\" network, self.current. First we zero out the optimizer gradients\n",
    "    # and then we apply the update step using qs_selected and qs_target.\n",
    "    self.optimizer.zero_grad()\n",
    "    loss = (torch.nn.functional.mse_loss(qs_selected, qs_target)).mean()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return torch.mean(td_error).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d4345",
   "metadata": {},
   "source": [
    "The cell below contains all of the tunable hyper-parameters of the DQNAgent. Once you have a working implementation of the two exercises above, you can tune these hyper-parameters and see how they affect the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2750225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### HYPER PARAMETERS ##########\n",
    "# ### EPISODE ###################\n",
    "# Number of episodes to train for\n",
    "EPISODES = 250\n",
    "# Number of steps per episode\n",
    "STEPS = 200\n",
    "\n",
    "# ### TRAINING ##################\n",
    "# Start exploration rate\n",
    "# for eps greedy policy\n",
    "EPSILON_START = 1\n",
    "# End exploration rate\n",
    "EPSILON_END = 0.001\n",
    "# Exploration rate decay\n",
    "# per EPISODE\n",
    "EPSILON_DECAY = 0.985\n",
    "# Discount rate\n",
    "GAMMA = 0.95\n",
    "# DQN Target network update\n",
    "# frequency in EPISODES\n",
    "TARGET_FREQ = 4\n",
    "# DQN live network update\n",
    "# frequency in STEPS\n",
    "UPDATE_FREQ = 1\n",
    "# Learning rate\n",
    "LR = 0.00015\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# ### NETWORK ARCHITECTURE ######\n",
    "# Number of hidden layers\n",
    "# of H units\n",
    "NUM_H = 2\n",
    "# Number of units in hidden\n",
    "# layers\n",
    "H = 64\n",
    "# ### HYPER PARAMETERS END #######\n",
    "run_name = create_run_name(\n",
    "        alg='DQN',\n",
    "        env='stick',\n",
    "        num_layers=NUM_H,\n",
    "        hidden_dim=H,\n",
    "        eps_start=EPSILON_START,\n",
    "        eps_end=EPSILON_END,\n",
    "        decay=EPSILON_DECAY,\n",
    "        gamma=GAMMA,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        num_ep=EPISODES,\n",
    "        num_step=STEPS,\n",
    "        updt_freq=UPDATE_FREQ,\n",
    "        sw_freq=TARGET_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5339a",
   "metadata": {},
   "source": [
    "The code in the cell below encapsulates the DQNAgent and controls its training process according to the algorithm presented in the introductiory section. We start with constructing the environment **env**, and the DQNAgent itself. We then continue with the implementation of the overall DQN algorithm from the introductory section.\n",
    "\n",
    "We copy the algorithm below, for your ease of reference:\n",
    "\n",
    "1. Initialize replay memory (buffer) $D$ to capacity $N$\n",
    "2. Initialize *live* action-value function neural network $Q$ with random weights.\n",
    "3. Initialize *target* action-value function neural network $\\hat{Q}$ by copying weights from the *live* one. \n",
    "4. **for** episode = $1$, $M$ **do**\n",
    "    1. Restart episode and initialize start state $s_1$\n",
    "    2. **for** t = $1$, $T$ **do**\n",
    "        1. With probability $\\varepsilon$ select random action $a_t$, otherwise <font color=\"green\"> select $a_t = \\max_aQ(s_t, a_t; \\theta)$ </font>\n",
    "        2. Execute selected action $a_t$ in the environment, and get $s_{t+1}$, $r_t$ and $\\eta_{t+1}$, where $\\eta_{t+1}$ indicates whether $s_{t+1}$ is a terminal state.\n",
    "        3. Store *transition* $(s_j, a_j, s_{j+1}, r_j, \\eta_{j+1})$ in replay memory (buffer) $D$.\n",
    "        4. Sample mini-batch of transitions of size $h$ from replay memory $D$.\n",
    "        5. <font color=\"green\"> Calculate target value $y_j$ using the following rule: $y_j = r_j + (1 - \\eta_{j+1})\\gamma\\max_{a'}\\hat{Q}(s_{j+1}, a; \\theta))$ </font>\n",
    "        6. Calculate temoral difference error *TD Error*: $\\delta = y_j - Q(s_j, a_j; \\theta)$\n",
    "        7. Regress the *live* network $Q$ with respect to $(\\delta)^2$\n",
    "    4. **end for**\n",
    "5. **end for**\n",
    "\n",
    "We encourage you to find each pseudo-code step in the implementation below before starting with the exercises. Finally, when you complete both of the exercises, run this cell to start the training process!\n",
    "\n",
    "**NOTE!**: Remember to run all previous cells. The final output of the cell below will be a figure displaing returns (rewards obtained throughout episodes), and TD Errors per each step when DQN agent was updated (each backward() pass). You can review theese figures by clicking on Jupyter logo in top left corner, and navigating to **tmp_deep_q_learning** folder, where figures are stored and named based on the algorithm hyper-parameters that have generated them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a09c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvWrapper(gym_env=gym.make('CartPole-v0'), steps=STEPS)\n",
    "# Initialize Q networks, replay memory\n",
    "agent = DQNAgent(\n",
    "    state_size=env.state_size(),\n",
    "    action_size=env.action_size(),\n",
    "    gamma=GAMMA,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    num_hidden=NUM_H,\n",
    "    hidden_units=H\n",
    ")\n",
    "\n",
    "epsilon = EPSILON_START\n",
    "results = []\n",
    "td_errors = []\n",
    "start = time.time()\n",
    "random.seed(0)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # Start game/episode\n",
    "    state = env.reset()\n",
    "    cum_rew = 0\n",
    "\n",
    "    if episode > 10 and episode % TARGET_FREQ == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    # Loop inside one game episode\n",
    "    for t in range(STEPS):\n",
    "        \n",
    "        # Display the game. Comment bellow line in order to get faster training.\n",
    "        env.render()\n",
    "        \n",
    "        if random.random() <= epsilon:\n",
    "            action = env.env.action_space.sample()\n",
    "        else:\n",
    "            # ########## EXERCISE 1: ACTION SELECTION ##########################\n",
    "            action = agent.select_action(state=torch.from_numpy(state).detach())\n",
    "            # ##################################################################\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        cum_rew += reward\n",
    "        \n",
    "        # Store transition to replay memory\n",
    "        agent.remember(state=state, action=action, reward=reward, next_state=next_state, done=float(done))\n",
    "        \n",
    "        # Update Q-values\n",
    "        if episode > 10 and (episode + t) % UPDATE_FREQ == 0:\n",
    "            # ## EXERCISE 2: Q-VALUE UPDATE ##\n",
    "            td_error = agent.backward()\n",
    "            # ################################\n",
    "            td_errors.append(td_error)\n",
    "\n",
    "        if done or (t == STEPS - 1):\n",
    "            if episode > 10:\n",
    "                print(\"EPISODE: {0: <4}/{1: >4} | EXPLORE RATE: {2: <7.4f} | SCORE: {3: <7.1f}\"\n",
    "                      \" | TD ERROR: {4: <5.2f} \".format(episode + 1, EPISODES, epsilon, cum_rew, td_error))\n",
    "            else:\n",
    "                print(\"EPISODE: {0: <4}/{1: >4} | EXPLORE RATE: {2: <7.4f} | SCORE: {3: <7.1f}\"\n",
    "                      \" | WARMUP - NO TD ERROR\".format(episode + 1, EPISODES, epsilon, cum_rew))\n",
    "            results.append(cum_rew)\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if epsilon > EPSILON_END:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "\n",
    "print()\n",
    "print(\"TOTAL EPISODES: {0: <4} | TOTAL UPDATE STEPS: {1: <7} | TOTAL TIME [s]: {2: <7.2f}\"\n",
    "      .format(EPISODES, len(td_errors), total_time))\n",
    "print(\"EP PER SECOND: {0: >10.6f}\".format(total_time / EPISODES))\n",
    "print(\"STEP PER SECOND: {0: >8.6f}\".format(total_time / len(td_errors)))\n",
    "\n",
    "fig = visualize_result(\n",
    "    returns=results,\n",
    "    td_errors=td_errors,\n",
    "    policy_errors=None\n",
    ")\n",
    "fig.savefig(os.path.join(STORE_PATH, run_name + '.png'), dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0890b2e",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "The end goal of this exercise is for you to get the feel of applying RL concepts in practice. To that effect, try to figure out an answer to questions below:\n",
    "\n",
    "1. Which hyper-parameters affect training stability and why?\n",
    "2. Which hyper-parameters affect the potential to obtain high rewards?\n",
    "3. What is the reasonable scale of TD error in this particular problem?\n",
    "\n",
    "\n",
    "Finally,you can try to solve other problems using DQNs, provided they have discrete action space. You can find additional pre-made environments on the [OpenAI Gym website](https://gym.openai.com/envs/#classic_control). If you wish to try this notebook on other problems, simply substitute the environment name on line 1 of the cell above with the new one. (e.g. **CartPole-v0** -> **MountainCar-v0**)\n",
    "\n",
    "Additionally, we provide a couple of resources for further study:\n",
    "\n",
    "* Stability of the DQN algorithm is analyzed in greater detail by [Roderick et al.: Implementing the Deep Q-Network](https://arxiv.org/abs/1711.07478). You can find the original DQN paper by Mnih et al. [here](https://arxiv.org/abs/1312.5602). Finally, if you are interested in further study into DQNs, we recommend two additional papers: [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) by Hasselt et al, and [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) by Wang et al. Many of the open-source DQN implementations actually use both of these papers to further improve the base algorithm!\n",
    "* Couple of open-source implementations of DQN (and other algorithms!) are: [Baselines](https://github.com/openai/baselines), [Spinning Up](https://github.com/openai/spinningup), [rllab](https://github.com/rll/rllab). \n",
    "* A great blog to read about various RL (and ML) papers: [Lil'Log](https://lilianweng.github.io/lil-log/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
